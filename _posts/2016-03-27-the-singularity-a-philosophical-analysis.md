---
inFeed: true
hasPage: true
inNav: true
inLanguage: null
starred: true
keywords: []
description: ''
datePublished: '2016-03-29T01:28:36.831Z'
dateModified: '2016-03-29T01:28:31.789Z'
title: |
  The Singularity: A Philosophical Analysis
author: []
sourcePath: _posts/2016-03-27-the-singularity-a-philosophical-analysis.md
published: true
authors: []
publisher:
  name: null
  domain: null
  url: null
  favicon: null
url: the-singularity-a-philosophical-analysis/index.html
_type: Article

---
# The Singularity: A Philosophical Analysis

What happens when machines become more intelligent than humans? One view is that this event
will be followed by an explosion to ever-greater levels of intelligence, as each generation of machines creates more intelligent machines in turn. This intelligence explosion is now often known
as the "singularity".

The basic argument here was set out by the statistician I. J. Good in his 1965 article "Specula-
tions Concerning the First Ultraintelligent Machine":

> Let an ultraintelligent machine be defined as a machine that can far surpass all the
> intellectual activities of any man however clever. Since the design of machines is one
> of these intellectual activities, an ultraintelligent machine could design even better
> machines; there would then unquestionably be an "intelligence explosion", and the
> intelligence of man would be left far behind. Thus the first ultraintelligent machine is
> the last invention that man need ever make.
> 

The key idea is that a machine that is more intelligent than humans will be better than humans
at designing machines. So it will be capable of designing a machine more intelligent than the most
intelligent machine that humans can design. So if it is itself designed by humans, it will be capable
of designing a machine more intelligent than itself. By similar reasoning, this next machine will also be capable of designing a machine more intelligent than itself. If every machine in turn does
what it is capable of, we should expect a sequence of ever more intelligent machines.

This intelligence explosion is sometimes combined with another idea, which we might call the
"speed explosion". The argument for a speed explosion starts from the familiar observation that
computer processing speed doubles at regular intervals. Suppose that speed doubles every two
years and will do so indefinitely. Now suppose that we have human-level artificial intelligence
designing new processors. Then faster processing will lead to faster designers and an ever-faster
design cycle, leading to a limit point soon afterwards.

The argument for a speed explosion was set out by the artificial intelligence researcher Ray
Solomono in his 1985 article "The Time Scale of Artificial Intelligence". Eliezer Yudkowsky
gives a succinct version of the argument in his 1996 article "Staring at the Singularity": 
> 
> Computing speed doubles every two subjective years of work. Two years after Ar-
> tificial Intelligences reach human equivalence, their speed doubles. One year later,
> their speed doubles again. Six months - three months - 1.5 months ... Singularity.

The intelligence explosion and the speed explosion are logically independent of each other. In
principle there could be an intelligence explosion without a speed explosion and a speed explosion
without an intelligence explosion. But the two ideas work particularly well together. Suppose
that within two subjective years, a greater-than-human machine can produce another machine that
is not only twice as fast but 10% more intelligent, and suppose that this principle is indefinitely
extensible. Then within four objective years there will have been an infinite number of generations,
with both speed and intelligence increasing beyond any finite level within a finite time. This
process would truly deserve the name "singularity".

Of course the laws of physics impose limitations here. If the currently accepted laws of rela-
tivity and quantum mechanics are correct---or even if energy is finite in a classical universe---then
we cannot expect the principles above to be indefinitely extensible. But even with these physi-
cal limitations in place, the arguments give some reason to think that both speed and intelligence
might be pushed to the limits of what is physically possible. And on the face of it, it is unlikely
that human processing is even close to the limits of what is physically possible. So the arguments suggest that both speed and intelligence might be pushed far beyond human capacity in a relatively
short time. This process might not qualify as a "singularity" in the strict sense from mathematics
and physics, but it would be similar enough that the name is not altogether inappropriate.

The term "singularity" was introduced by the science fiction writer Vernor Vinge in a 1983
opinion article. It was brought into wider circulation by Vinge's influential 1993 article "The
Coming Technological Singularity" and by the inventor and futurist Ray Kurzweil's popular 2005
book The Singularity is Near. In practice, the term is used in a number of different ways. A loose
sense refers to phenomena whereby ever-more-rapid technological change leads to unpredictable
consequences. A very strict sense refers to a point where speed and intelligence go to infinity, as
in the hypothetical speed/intelligence explosion above. Perhaps the core sense of the term, though,
is a moderate sense in which it refers to an intelligence explosion through the recursive mechanism
set out by I. J. Good, whether or not this intelligence explosion goes along with a speed explosion
or with divergence to infinity. I will always use the term "singularity" in this core sense in what
follows.